{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import sample\n",
    "import random\n",
    "import gzip\n",
    "import gzip\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(78263992)\n",
    "\n",
    "# specify device depending on availability of GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wraps a separated dataset to avoid issues while shuffling\n",
    "class WrapperDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "# get the same batch split for inputs and labels\n",
    "def get_batched_x_y(x, y, batch_size, shuffle):\n",
    "    dataset = WrapperDataset(x, y)\n",
    "    batched = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return batched\n",
    "\n",
    "def getMalicious(X, y):\n",
    "    X_mal = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            X_mal.append(X[i])\n",
    "    X_mal = torch.stack(X_mal)\n",
    "    y_mal = y[y.nonzero()].squeeze(1)\n",
    "    \n",
    "    return X_mal, y_mal\n",
    "\n",
    "def zeroPadInput(inputs, amount=50):\n",
    "    # adds zero padding vectors to the end of each sequence\n",
    "    for i in range(len(inputs)):\n",
    "        zero_padding = torch.zeros(inputs[i].shape[0], amount, inputs[i].shape[2])\n",
    "        inputs[i] = torch.cat([inputs[i], zero_padding], dim=1)\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load(\"X_train.pt\").to_dense()\n",
    "X_val = torch.load(\"X_val.pt\").to_dense()\n",
    "X_test = torch.load(\"X_test.pt\").to_dense()\n",
    "y_train = torch.load(\"y_train.pt\")\n",
    "y_val = torch.load(\"y_val.pt\")\n",
    "y_test = torch.load(\"y_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = zeroPadInput([X_train, X_val, X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "lstm_layers = 1\n",
    "epochs = 100\n",
    "train_batch_size = 256\n",
    "early_stopping_patience = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[2]\n",
    "sequence_size = X_train.shape[1]\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, lstm_layers, pooling):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=True)\n",
    "        self.pool = pooling(sequence_size)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out = self.lstm(x)[0]\n",
    "        pool_out = self.pool(lstm_out.permute(0, 2, 1))\n",
    "        linear_out = self.linear(pool_out.squeeze())\n",
    "        return torch.squeeze(linear_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "    myX = x.to(device)\n",
    "    output = model(myX)\n",
    "    return output > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lstm): LSTM(264, 128, batch_first=True)\n",
       "  (pool): MaxPool1d(kernel_size=150, stride=150, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initModel(input_size, hidden_size, lstm_layers, pooling):\n",
    "    model = Net(input_size, hidden_size, lstm_layers, pooling)\n",
    "    model.to(device)\n",
    "    return model\n",
    "model = initModel(input_size, hidden_size, lstm_layers, nn.MaxPool1d)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, y, model, batch_size, loss_function):\n",
    "    averageLoss = 0.0\n",
    "    averageSensitivity = 0.0\n",
    "    averageSpecificity = 0.0\n",
    "    numBatches = 0\n",
    "    # batch the data set\n",
    "    batched = get_batched_x_y(x, y, batch_size, False)\n",
    "    for xi, yi in batched:\n",
    "        # move data to device\n",
    "        xi = xi.to(device)\n",
    "        yi = yi.to(device)\n",
    "        # forward pass through model\n",
    "        output = model(xi)\n",
    "        # calculate current loss of model\n",
    "        loss = loss_function(output, yi)\n",
    "        # calculate measures\n",
    "        predicted = output > 0\n",
    "        matches = yi == predicted\n",
    "        sensitivity = matches[yi == 1].sum() / (yi == 1).sum()\n",
    "        specificity = matches[yi == 0].sum() / (yi == 0).sum()\n",
    "        # record all values\n",
    "        averageSensitivity += sensitivity.item()\n",
    "        averageSpecificity += specificity.item()\n",
    "        averageLoss += loss.item()\n",
    "        numBatches += 1\n",
    "    averageSensitivity /= numBatches\n",
    "    averageSpecificity /= numBatches\n",
    "    averageLoss /= numBatches\n",
    "    measures = (averageSensitivity, averageSpecificity)\n",
    "    return measures, averageLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lstm): LSTM(264, 128, batch_first=True)\n",
       "  (pool): MaxPool1d(kernel_size=150, stride=150, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('final-lstm-model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 0.073531 Sensitivity 0.976 Specificity 0.935\n"
     ]
    }
   ],
   "source": [
    "test_measures, test_loss = test(X_test, y_test, model, len(y_test), loss_function)\n",
    "print(\"Test Loss {:.6f} Sensitivity {:.3f} Specificity {:.3f}\"\n",
    "         .format(test_loss, test_measures[0], test_measures[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lstm): LSTM(264, 128, batch_first=True)\n",
       "  (pool): MaxPool1d(kernel_size=150, stride=150, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create API dictionary\n",
    "idx = torch.tensor(range(0, 264))\n",
    "api_dict = torch.zeros(len(idx), idx.max()+1).scatter_(1, idx.unsqueeze(1), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insertion of api into sequence\n",
    "def insert_api(seq, api, i):\n",
    "    api = api.to(device)\n",
    "    new_seq = torch.cat((torch.cat((seq[:i],api.unsqueeze(0))),seq[i:-1]))\n",
    "    return new_seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-1b10f05afa54>:25: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:962.)\n",
      "  y_mal = y[y.nonzero()].squeeze(1)\n"
     ]
    }
   ],
   "source": [
    "X_val_mal, y_val_mal = getMalicious(X_val, y_val)\n",
    "batched_data = get_batched_x_y(X_val_mal, y_val_mal, 64, False)\n",
    "X_val_mal = []\n",
    "for xi, yi in batched_data:\n",
    "    X_val_mal.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batched version (jacobian is calculated at every batch instead of every sample)\n",
    "def advGeneration(X_init, model, surrogate_model):\n",
    "    for m in range(len(X_init)):\n",
    "        xi = X_init[m].clone()\n",
    "        xi.requires_grad = True\n",
    "        xi = xi.to(device)\n",
    "        jacobian = torch.diagonal(torch.autograd.functional.jacobian(surrogate_model.forward, xi)).permute(2, 0, 1)\n",
    "        for i in range(xi.shape[0]):\n",
    "            x_sample = xi[i]\n",
    "            counter = 0   \n",
    "            while predict(x_sample.unsqueeze(0), model)and counter<50:\n",
    "                random_position = random.randint(0,torch.count_nonzero(x_sample.detach())-1)\n",
    "                    #check if sample is predicted as benign\n",
    "                api_difs = []\n",
    "                for api in api_dict:\n",
    "                    #insert API into sequence\n",
    "                    new_seq = insert_api(x_sample.squeeze(), api, random_position)\n",
    "                    #norm of difference between new sequence and jacobian\n",
    "                    dif = torch.norm(torch.sign(x_sample) - torch.sign(jacobian[i]))\n",
    "                    api_difs.append((dif,new_seq))\n",
    "                min_seq = min(api_difs, key = lambda t: t[0])\n",
    "                #replace sequence\n",
    "                xi[i] = min_seq[1]\n",
    "                counter += 1\n",
    "            #replace batch\n",
    "            X_init[m] = xi\n",
    "    return X_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitebox Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2\n",
      "9\n",
      "32\n",
      "2\n",
      "0\n",
      "10\n",
      "17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e7d47b993c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwhitebox_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madvGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_mal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-f17bb286c6b0>\u001b[0m in \u001b[0;36madvGeneration\u001b[0;34m(X_init, model, surrogate_model)\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mnew_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minsert_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0;31m#norm of difference between new sequence and jacobian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     \u001b[0mdif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                     \u001b[0mapi_difs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdif\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mmin_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_difs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "whitebox_adv = advGeneration(X_val_mal, model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(whitebox_adv, 'whitebox_adv_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('whitebox_adv_val.pt', 'rb') as f_in:\n",
    "    with gzip.open('whitebox_adv_val.gz', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lstm_layers = 2\n",
    "training_epochs = 10\n",
    "augmentation_epochs = 3\n",
    "train_batch_size = 256\n",
    "epsilon = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, lstm_layers, pooling):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=True)\n",
    "        self.pool = pooling(sequence_size)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out = self.lstm(x)[0]\n",
    "        pool_out = self.pool(lstm_out.permute(0, 2, 1))\n",
    "        linear_out = self.linear(pool_out.squeeze())\n",
    "        return torch.squeeze(linear_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lstm): LSTM(264, 256, num_layers=2, batch_first=True)\n",
       "  (pool): MaxPool1d(kernel_size=150, stride=150, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initModel(input_size, hidden_size, lstm_layers, pooling):\n",
    "    model = Net(input_size, hidden_size, lstm_layers, pooling)\n",
    "    model.to(device)\n",
    "    return model\n",
    "surrogateModel = initModel(input_size, hidden_size, lstm_layers, nn.MaxPool1d)\n",
    "surrogateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surrogateModel.load_state_dict(torch.load(\"surrogate-model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackbox Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "6\n",
      "19\n",
      "12\n",
      "7\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c2843390db66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblackbox_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madvGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_mal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogateModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-f17bb286c6b0>\u001b[0m in \u001b[0;36madvGeneration\u001b[0;34m(X_init, model, surrogate_model)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapi_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0;31m#insert API into sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     \u001b[0mnew_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minsert_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0;31m#norm of difference between new sequence and jacobian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mdif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-35c67d8015fa>\u001b[0m in \u001b[0;36minsert_api\u001b[0;34m(seq, api, i)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minsert_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnew_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "blackbox_adv = advGeneration(X_val_mal, model, surrogateModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(blackbox_adv, 'blackbox_adv_val.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('blackbox_adv_val.pt', 'rb') as f_in:\n",
    "    with gzip.open('blackbox_adv_val.gz', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackbox and whitebox performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failed_attacks(adv_samples, model):\n",
    "    samples = adv_samples.to(device)\n",
    "    prediction = model(samples)\n",
    "    return (prediction >= 0).cpu()\n",
    "    \n",
    "def count_added_api(adv_samples):\n",
    "    added_api = []\n",
    "    for seq in adv_samples:\n",
    "        added_api_counter = 0\n",
    "        for api in seq:\n",
    "            if (len(torch.nonzero(api) == 0)):\n",
    "                added_api_counter += 1\n",
    "        added_api.append(added_api_counter-100)\n",
    "    return np.array(added_api)\n",
    "\n",
    "def remove_zeros_and_failed(api_counts, failed_attacks):\n",
    "    return api_counts[torch.logical_and(torch.tensor(api_counts != 0), torch.logical_not(failed_attacks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitebox_adv =  torch.load('whitebox_adv_val.pt') \n",
    "blackbox_adv =  torch.load('blackbox_adv_val.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitebox_adv = torch.cat(whitebox_adv).cpu()\n",
    "blackbox_adv = torch.cat(blackbox_adv).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printOverhead(name, adv_samples, model):\n",
    "    failed = get_failed_attacks(adv_samples, model)\n",
    "    count_api = count_added_api(adv_samples)\n",
    "    count_filtered = remove_zeros_and_failed(count_api, failed)\n",
    "    overhead = np.mean(count_filtered/100)\n",
    "    print(name, \"overhead:\", str(overhead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitebox attack overhead: 0.17072665203608878\n",
      "Blackbox attack overhead: 0.16809117503656754\n"
     ]
    }
   ],
   "source": [
    "printOverhead(\"Whitebox attack\", whitebox_adv, model)\n",
    "printOverhead(\"Blackbox attack\", blackbox_adv, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEffectiveness(name, adv_samples, model):\n",
    "    failed = get_failed_attacks(adv_samples, model)\n",
    "    count_api = count_added_api(adv_samples)\n",
    "    count_filtered = remove_zeros_and_failed(count_api, failed)\n",
    "    effectiveness = torch.logical_not(failed[count_api != 0]).sum()/(count_api != 0).sum()\n",
    "    effectiveness = effectiveness.item()\n",
    "    print(name, \"effectiveness:\", str(effectiveness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitebox attack effectiveness: 0.9834532141685486\n",
      "Blackbox attack effectiveness: 0.9836930632591248\n"
     ]
    }
   ],
   "source": [
    "printEffectiveness(\"Whitebox attack\", whitebox_adv, model)\n",
    "printEffectiveness(\"Blackbox attack\", blackbox_adv, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "blackbox_count_filtered = remove_zeros_and_failed(count_added_api(whitebox_adv), get_failed_attacks(whitebox_adv, model))\n",
    "whitebox_count_filtered = remove_zeros_and_failed(count_added_api(blackbox_adv), get_failed_attacks(blackbox_adv, model))\n",
    "plt.hist(blackbox_count_filtered, bins=50, color = 'r', label = \"Black-box attack\",alpha=0.5)\n",
    "plt.hist(whitebox_count_filtered, bins=50, color = 'b',label = \"White-box attack\", alpha=0.5)\n",
    "plt.ylabel(\"number of samples\")\n",
    "plt.xlabel(\"number of added API calls\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Adversarial Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_mal, y_val_mal = getMalicious(X_val, y_val)\n",
    "batched_data = get_batched_x_y(X_val_mal, y_val_mal, 64, False)\n",
    "X_val_mal = []\n",
    "for xi, yi in batched_data:\n",
    "    X_val_mal.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batched version (jacobian is calculated at every batch instead of every sample)\n",
    "def advGenerationRandom(X_init, model):  \n",
    "    for m in range(len(X_init)):\n",
    "        xi = X_init[m]\n",
    "        xi = xi.to(device)\n",
    "        for i in range(xi.shape[0]):\n",
    "            x_sample = xi[i]\n",
    "            counter = 0\n",
    "            while predict(x_sample.unsqueeze(0), model) and counter<50:\n",
    "                random_position = random.randint(0,torch.count_nonzero(x_sample.detach())-1)\n",
    "                #insert API into sequence\n",
    "                new_seq = insert_api(x_sample.squeeze(), random.choice(api_dict), random_position)\n",
    "                #replace sequence\n",
    "                counter += 1\n",
    "                xi[i] = new_seq\n",
    "            #replace batch\n",
    "            X_init[m] = xi\n",
    "    return X_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_adv = advGenerationRandom(X_val_mal, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random attack performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_adv = torch.cat(random_adv).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random attack effectiveness: 0.07074340432882309\n"
     ]
    }
   ],
   "source": [
    "printEffectiveness(\"Random attack\", random_adv, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks against hardened models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardened LSTM model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "lstm_layers = 1\n",
    "epochs = 100\n",
    "train_batch_size = 256\n",
    "early_stopping_patience = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[2]\n",
    "sequence_size = X_train.shape[1]\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, lstm_layers, pooling):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=True)\n",
    "        self.pool = pooling(sequence_size)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out = self.lstm(x)[0]\n",
    "        pool_out = self.pool(lstm_out.permute(0, 2, 1))\n",
    "        linear_out = self.linear(pool_out.squeeze())\n",
    "        return torch.squeeze(linear_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lstm): LSTM(264, 128, batch_first=True)\n",
       "  (pool): MaxPool1d(kernel_size=150, stride=150, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initModel(input_size, hidden_size, lstm_layers, pooling):\n",
    "    model = Net(input_size, hidden_size, lstm_layers, pooling)\n",
    "    model.to(device)\n",
    "    return model\n",
    "hardenedLSTMModel = initModel(input_size, hidden_size, lstm_layers, nn.MaxPool1d)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardenedLSTMModel.load_state_dict(torch.load(\"hardened-lstm-model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardened whitebox attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mal, y_test_mal = getMalicious(X_test, y_test)\n",
    "batched_data = get_batched_x_y(X_test_mal, y_test_mal, 64, False)\n",
    "X_test_mal = []\n",
    "for xi, yi in batched_data:\n",
    "    X_test_mal.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardened_whitebox_adv = advGeneration(X_test_mal, hardenedLSTMModel, hardenedLSTMModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hardened_whitebox_adv, 'hardened_whitebox_adv_test.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardened whitebox performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardened_whitebox_adv = torch.load('hardened_whitebox_adv_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardened_whitebox_adv = torch.cat(hardened_whitebox_adv).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardened Whitebox attack effectiveness: 0.007045797538012266\n"
     ]
    }
   ],
   "source": [
    "printEffectiveness(\"hardened Whitebox attack\", hardened_whitebox_adv, hardenedLSTMModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardened surrogate model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lstm_layers = 2\n",
    "training_epochs = 10\n",
    "augmentation_epochs = 3\n",
    "train_batch_size = 256\n",
    "epsilon = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, lstm_layers, pooling):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=True)\n",
    "        self.pool = pooling(sequence_size)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out = self.lstm(x)[0]\n",
    "        pool_out = self.pool(lstm_out.permute(0, 2, 1))\n",
    "        linear_out = self.linear(pool_out.squeeze())\n",
    "        return torch.squeeze(linear_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lstm): LSTM(264, 256, num_layers=2, batch_first=True)\n",
       "  (pool): MaxPool1d(kernel_size=150, stride=150, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initModel(input_size, hidden_size, lstm_layers, pooling):\n",
    "    model = Net(input_size, hidden_size, lstm_layers, pooling)\n",
    "    model.to(device)\n",
    "    return model\n",
    "hardenedSurrogateModel = initModel(input_size, hidden_size, lstm_layers, nn.MaxPool1d)\n",
    "hardenedSurrogateModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hardenedSurrogateModel.load_state_dict(torch.load(\"hardened-surrogate-model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardened blackbox attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardened_blackbox_adv = advGeneration(X_test_mal, hardenedLSTMModel, hardenedSurrogateModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hardened_blackbox_adv, 'hardened_blackbox_adv_test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardened blackbox performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardened_blackbox_adv = torch.load('hardened_blackbox_adv_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardened_blackbox_adv = torch.cat(hardened_blackbox_adv).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardened Blackbox attack effectiveness: 0.008807247504591942\n"
     ]
    }
   ],
   "source": [
    "printEffectiveness(\"hardened Blackbox attack\", hardened_blackbox_adv, hardenedLSTMModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardened random attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mal, y_test_mal = getMalicious(X_test, y_test)\n",
    "batched_data = get_batched_x_y(X_test_mal, y_test_mal, 64, False)\n",
    "X_test_mal = []\n",
    "for xi, yi in batched_data:\n",
    "    X_test_mal.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardened_random_adv = advGenerationRandom(X_test_mal, hardenedLSTMModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardened random performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardened_random_adv = torch.cat(hardened_random_adv).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardened Random attack effectiveness: 0.012833417393267155\n"
     ]
    }
   ],
   "source": [
    "printEffectiveness(\"hardened Random attack\", hardened_random_adv, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
